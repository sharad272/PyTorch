ML 
Supervised (In-Depth)
Training/Testing

Inference

Types of Prompt Engineering
Zero-Shot
Few-Shot

Reranking (BERT/Cross encoder)
Bi-Encoder finds â†’ Cross-Encoder refines â†’ LLM answers

Failures of RAG
- Inaccurate Response
- No Response

ANN/HNSW

Architecture of Projects

Types of Vector DB and its working!!
Embedding!! (Cosine Similarity and Euclidian distance)

AWS/Azure Working (Deploying of LLMs)

Chunking: Data Split and its types

Techniques to avoid hallucinations

Evaluation metrics 

https://www.uber.com/en-IN/blog/powering-billion-scale-vector-search-with-opensearch/



Quantization/Distillation.






Core GenAI Fundamentals

Focus on Frameworks

Foundation models

Generative vs discriminative models

Tokens & tokenization

Context window limitations

Temperature, top-p, top-k

Deterministic vs stochastic outputs

Hallucinations & causes

ğŸ§¾ Prompting Concepts

System vs user vs assistant prompts

Zero-shot vs few-shot prompting

Prompt templates

Prompt versioning

Instruction grounding

Prompt injection attacks

Prompt regression testing

ğŸ” Embeddings & Semantic Search

Text embeddings

Vector representations

Cosine similarity vs dot product vs Euclidean distance

Semantic similarity vs keyword search

Embedding dimensionality

Embedding drift

ğŸ“š RAG (Retrieval-Augmented Generation)

RAG architecture

Chunking strategies

Chunk size vs overlap tradeoffs

Metadata filtering

Top-k retrieval

Hybrid search (semantic + keyword)

Context injection strategies

Retrieval recall vs precision

RAG failure modes

ğŸ—‚ï¸ Vector Databases

Indexing strategies

Approximate vs exact nearest neighbors

Index rebuild vs incremental updates

Recallâ€“latency tradeoff

Vector + metadata queries

â˜ï¸ Managed LLM Platforms (Conceptual)

LLM-as-a-service

Model selection tradeoffs

Rate limiting & quotas

Token-based pricing

Latency vs cost tradeoffs

Data privacy guarantees

IAM-based access control

âš™ï¸ GenAI Application Architecture

Sync vs async inference

API-based inference services

Retry & timeout strategies

Request batching

Response caching

Streaming responses

ğŸ“Š Evaluation & Quality

Faithfulness

Groundedness

Relevance

Retrieval accuracy

Human vs automated evaluation

Golden dataset evaluation

Prompt A/B testing

ğŸ“‰ Monitoring & Observability

Latency monitoring

Token usage monitoring

Error rates

Retrieval failure detection

Output quality degradation

Cost monitoring

ğŸ” Safety & Guardrails

Prompt injection risks

Jailbreak attempts

Content moderation

Output filtering

Input sanitization

ğŸ§ª Testing GenAI Systems

Prompt regression tests

RAG consistency testing

Edge-case testing

Failure logging

âš–ï¸ Performance & Cost Optimization

Context length optimization

Chunk reduction strategies

Caching strategies

Model size vs latency tradeoffs

Cost vs accuracy tradeoffs

ğŸ§  GenAI System Design (Conceptual)

When to use RAG vs fine-tuning

When managed LLMs are preferred

Failure handling strategies

Scaling inference workloads


Multi-tenant GenAI systems.



Optuna: An Open Source framework for hyperparameter tuning of the NN model.

Optuna is an open-source hyperparameter optimization framework used mainly in machine learning and deep learning to automatically find the best hyperparameters for your models â€” faster and smarter than manual tuning or grid search.


When you train ML/DL models, you constantly ask:

What learning rate?

How many layers?

Batch size?

Dropout?

Regularization strength?

Trying all combinations manually or with grid search is slow and wasteful.
Optuna intelligently explores the search space and converges faster.



You define an objective function (what to minimize/maximize, e.g. validation loss)

Optuna suggests a set of hyperparameters

Your model trains with those parameters

Optuna observes the result

It uses smart sampling to suggest better parameters next time

Bad trials can be stopped early (pruning)






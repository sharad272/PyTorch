Training Process of the neural network:
1- Forward Pass --> Compute the output of the network
2- Loss Function --> Calculate the Loss Function 
3- Backward Pass (BackPropogation) --> Compute the gradients of the Loss with respect to parameters
4- Update gradients --> Adjust the parameters using an optimisation algorithm (e.g. gradient descent)




X ------> W -------> Activation function (Sigmoid) -----> y_prediction ---> Loss
(Input)   (Weight)          bias


Forward Pass Computation:
1- Linear Transformation
   z= weight*x(input) + bias

2- Activation function (Sigmoid):
     yprediction 



Loss Function (Binary Cross- Entropy Level)


Neural Network is just like a nested functions of derivatives which manually done is almost impossible.

That's why we need autograd which automatic compute the derivatives for the tensor operations.



PyTorch creates computation graph when backward() (Calculate derivatives) function is being called.


Start with random weights â†’ see how wrong you are â†’ use gradients to adjust â†’ repeat until the error canâ€™t be reduced anymore.

We will stop training the NN until the loss function ~ 0 (minimal) or gets flattens i.e. gives the same value.

When gradients are tiny and loss no longer improves on new data, thereâ€™s nothing useful left to learn.




How training really works (step by step)

1ï¸âƒ£ Initialize weights randomly
We donâ€™t know anything at the start, so:

ğ‘Š
âˆ¼
random
Wâˆ¼random

2ï¸âƒ£ Forward pass
Use current weights to compute prediction:

3ï¸âƒ£ Compute loss
Measure how wrong the prediction is:

4ï¸âƒ£ Compute gradients
Differentiate loss w.r.t. weights:


This tells us how to change weights to reduce loss.

5ï¸âƒ£ Update weights
	â€‹

6ï¸âƒ£ Repeat
Do this many times until: Loss function becomes mimimised




Why bias is needed (intuition)

Without bias:

ğ‘§ = ğ‘Šğ‘¥z Wx

This forces:

output = 0 when input = 0

decision boundary must pass through origin

Thatâ€™s very restrictive.

Bias lets the neuron:

activate even when inputs are zero

shift decision boundaries left/right



Simple example

With bias:

ğ‘¦ = 2ğ‘¥ + 3
y=2x+3

Without bias:

ğ‘¦=2ğ‘¥ 

y=2x

You canâ€™t represent +3
+3 without bias.

Bias lets the model fit data that doesnâ€™t pass through zero.

What bias really does

A neuron computes:

ğ‘§ = ğ‘Šğ‘¥ + ğ‘


If input 
x=0:   

then
z=b

So yes:

Bias determines the neuronâ€™s output when the input is zero.


We donâ€™t use sigmoid in hidden layers because its mathematical form forces the gradient toward zero when the input value becomes largely +ve  or -ve i.e. linear resprsentation which is a straight line only.


While activation function like Relu can detect non-linear representation which covers much of the info or data points.
